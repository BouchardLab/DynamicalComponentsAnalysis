{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge as RR\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from dca.data_util import load_sabes_data\n",
    "from dca.dca import DynamicalComponentsAnalysis as DCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality and neural data?\n",
    "Due to some combination of simple experimental paradigms and more fundamental unknown principles of cortical processing, the neural data we recorded is often much lower dimensional than the recording dimensionality. This tutorial explores a few different ways of thinking about neural dimensionality. It contrasts DCA with the commonly used PCA.\n",
    "\n",
    "# Run once to download the M1 reaching data\n",
    "The file is 1.1 GB, so it may take a few minutes to download. This data is from the Sabes lab and is recordings from M1 while a monkey is reaching to different locations on a grid. This is the same data used in the DCA paper. In this tutorial, we won't cross validate the results as was done in the paper to keep things simple.\n",
    "\n",
    "More information and data can be found here:\n",
    "\n",
    "O'Doherty, Joseph E., Cardoso, Mariana M. B., Makin, Joseph G., & Sabes, Philip N. (2017). Nonhuman Primate Reaching with Multichannel Sensorimotor Cortex Electrophysiology [Data set]. Zenodo. http://doi.org/10.5281/zenodo.583331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'indy_20160627_01.mat'\n",
    "if not os.path.isfile(fname): # check if file was already downloaded\n",
    "    tmp = f\"{fname}_tmp\"\n",
    "    urllib.request.urlretrieve('https://zenodo.org/record/583331/files/indy_20160627_01.mat?download=1', tmp)\n",
    "    os.rename(tmp, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's load and visualize some of the data\n",
    "We'll use 50ms bins and preprocess the data by removing neurons with very low firing rates, square-root transforming the spike counts, and high-pass filtering the data to remove slow nonstationarity (30s timescale).\n",
    "\n",
    "This should load a dictionary that contains the (preprocessed) spike counts for 109 neurons along with the cursor location sampled at the same rate. We'll visualize the spike raster and cursor location for 1 minute of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_sabes_data(fname, bin_width_s=.05, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = data.keys()\n",
    "print(data.keys())\n",
    "print(*[(key, data[key].shape) for key in keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['M1']\n",
    "Xn = X / X.std(axis=0, keepdims=True) # normalized version will be used later\n",
    "Y = data['cursor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(X[:1200].T, extent=[0, 1199*.05, 0, 108], cmap='gray_r', aspect='auto')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Neuron')\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(*Y[:1200].T, c='k')\n",
    "plt.xlabel('cursor x')\n",
    "plt.ylabel('cursor y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the dimensionality of the neural data?\n",
    "There are many ways of defining dimensionality. PCA organizes the data along projections of decreasing variance explained. If the variance of your dataset is confined to a lower dimensional subspace in the original measurement space, PCA will find this manifold. DCA, on the other hand, looks for subspaces with highly predictive dynamics, as measured by Predictive Information (PI).\n",
    "\n",
    "We'll focus on the first 30 dimensions, but you could extend the analysis out to 109. We'll look at the objective of PCA (variance explained) and DCA (PI) as a function of projection dimensionality. This is a purely unsupervised analysis of dimensionality. We can also ask how well the projections (found in an unsupervised manner) can be used to predict behavior for each method.\n",
    "\n",
    "One weakness of PCA, which motivated the development of DCA, is that it cannot distinguish high variance dynamics from high variance noise. Let's first look at the variance explained by PCA projections and their $R^2$ in predicting the behavioral data. We'll also plot the max $R^2$ for a fully supervised linear method at each dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dim = 30\n",
    "lag = 4 # 200ms lag for the neural data for predicting behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = np.arange(1, max_dim+1)\n",
    "pca_model = PCA()\n",
    "pca_model.fit(X)\n",
    "var = np.sum(pca_model.explained_variance_)\n",
    "\n",
    "pca1_scores = np.zeros(ds.size)\n",
    "max_scores = np.zeros(ds.size)\n",
    "for ii, d in enumerate(ds):\n",
    "    Xd = pca_model.transform(X)[:, :d]\n",
    "    rr_model = RR(alpha=1e-6)\n",
    "    rr_model.fit(Xd[:-lag], Y[lag:])\n",
    "    pca1_scores[ii] = r2_score(Y[lag:], rr_model.predict(Xd[:-lag]))\n",
    "rr_model = RR(alpha=1e-6)\n",
    "rr_model.fit(X[:-lag], Y[lag:])\n",
    "max_scores[:] = r2_score(Y[lag:], rr_model.predict(X[:-lag]))\n",
    "u, s, v = np.linalg.svd(rr_model.coef_)\n",
    "for ii, d in enumerate(range(1, Y.shape[1])):\n",
    "    rr_model.coef_ = (u[:, :d] * s[:d]) @ v[:d]\n",
    "    max_scores[ii] = r2_score(Y[lag:], rr_model.predict(X[:-lag]))\n",
    "\n",
    "\n",
    "plt.plot(ds, np.cumsum(pca_model.explained_variance_)[:ds.size] / var, label='Var. explained', c='C0')\n",
    "plt.ylim(0, 1.01)\n",
    "plt.plot(ds, pca1_scores / max_scores[-1], label=r'PCA $R^2$', c='C1')\n",
    "plt.plot(ds, max_scores / max_scores[-1], label=r'Max $R^2$', c='C3')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Projected dimensionality')\n",
    "plt.ylabel('0-1 normalized metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make visualization easier, we've normalized the y-axis so that 1 is the value of the metric at the full dimensionality of the dataset (109). So, all plots would eventually go to 1 at $d=109$.\n",
    "\n",
    "### Questions:\n",
    "- Does the variance explained look low dimensional?\n",
    "- Does the $R^2$ look low dimensional?\n",
    "\n",
    "We can run the equivalent analysis for DCA. Rather than explained variance, we'll look at PI. $R^2$ is computed in the same way across methods. This analysis will be somewhat slower since the projections need to be re-fit for each dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.zeros(ds.size)\n",
    "dca_scores = np.zeros(ds.size)\n",
    "dca_model = DCA(T=10, d=109)\n",
    "dca_model.estimate_data_statistics(X) # only need to estimate this once\n",
    "max_pi = dca_model.score() # PI of data with no dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ii, d in enumerate(ds):\n",
    "    print(d)\n",
    "    dca_model.fit_projection(d=d)\n",
    "    pi[ii] = dca_model.score()\n",
    "    Xd = dca_model.transform(X)\n",
    "    rr_model = RR(alpha=1e-6)\n",
    "    rr_model.fit(Xd[:-lag], Y[lag:])\n",
    "    dca_scores[ii] = r2_score(Y[lag:], rr_model.predict(Xd[:-lag]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds, pi / max_pi, label='PI', c='C0')\n",
    "plt.plot(ds, pca1_scores / max_scores[-1], label=r'PCA $R^2$', c='C1')\n",
    "plt.plot(ds, dca_scores / max_scores[-1], label=r'DCA $R^2$', c='C2')\n",
    "plt.plot(ds, max_scores / max_scores[-1], label=r'Max $R^2$', c='C3')\n",
    "\n",
    "plt.ylim(0, 1.01)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Projected dimensionality')\n",
    "plt.ylabel('0-1 normalized metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Does the PI look low dimensional?\n",
    "- Does the $R^2$ look low dimensional?\n",
    "\n",
    "# Preprocessing\n",
    "Data analysis methods are often sensitive to certain types of preprocessing. Spiking data is sometimes variance normalized per neuron before PCA is performed. Does this change the picture of the data PCA gives?\n",
    "\n",
    "What about DCA? What does this say about the invariances encoded in the choice of metrics: PI versus variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA()\n",
    "pca_model.fit(Xn) # Xn rather than X\n",
    "var = np.sum(pca_model.explained_variance_)\n",
    "\n",
    "pca2_scores = np.zeros(ds.size)\n",
    "max_scores2 = np.zeros(ds.size)\n",
    "for ii, d in enumerate(ds):\n",
    "    Xd = pca_model.transform(Xn)[:, :d]\n",
    "    rr_model = RR(alpha=1e-6)\n",
    "    rr_model.fit(Xd[:-lag], Y[lag:])\n",
    "    pca2_scores[ii] = r2_score(Y[lag:], rr_model.predict(Xd[:-lag]))\n",
    "rr_model = RR(alpha=1e-6)\n",
    "rr_model.fit(Xd[:-lag], Y[lag:])\n",
    "max_scores2[:] = r2_score(Y[lag:], rr_model.predict(Xd[:-lag]))\n",
    "u, s, v = np.linalg.svd(rr_model.coef_)\n",
    "for ii, d in enumerate(range(1, Y.shape[1])):\n",
    "    rr_model.coef_ = (u[:, :d] * s[:d]) @ v[:d]\n",
    "    max_scores2[ii] = r2_score(Y[lag:], rr_model.predict(Xd[:-lag]))\n",
    "\n",
    "plt.plot(ds, np.cumsum(pca_model.explained_variance_)[:ds.size] / var, label='Var. explained', c='C0')\n",
    "plt.ylim(0, 1.01)\n",
    "plt.plot(ds, pca2_scores / max_scores2[-1], label=r'PCA $R^2$', c='C1')\n",
    "plt.plot(ds, max_scores2 / max_scores2[-1], label=r'Max $R^2$', c='C3')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Projected dimensionality')\n",
    "plt.ylabel('0-1 normalized metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi2 = np.zeros(ds.size)\n",
    "dca_scores2 = np.zeros(ds.size)\n",
    "dca_model = DCA(T=10, d=109)\n",
    "dca_model.estimate_data_statistics(Xn) # only need to estimate this once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii, d in enumerate(ds):\n",
    "    print(d)\n",
    "    dca_model.fit_projection(d=d)\n",
    "    pi2[ii] = dca_model.score()\n",
    "    Xd = dca_model.transform(Xn)\n",
    "    rr_model = RR(alpha=1e-6)\n",
    "    rr_model.fit(Xd[:-lag], Y[lag:])\n",
    "    dca_scores[ii] = r2_score(Y[lag:], rr_model.predict(Xd[:-lag]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds, pi / max_pi, label='PI')\n",
    "plt.plot(ds, pca2_scores / max_scores2[-1], label=r'PCA $R^2$', c='C1')\n",
    "plt.plot(ds, dca_scores / max_scores2[-1], label=r'DCA $R^2$', c='C2')\n",
    "plt.plot(ds, max_scores2 / max_scores2[-1], label=r'Max $R^2$', c='C3')\n",
    "plt.ylim(0, 1.01)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Projected dimensionality')\n",
    "plt.ylabel('0-1 normalized metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for other simple visualization and analysis\n",
    "\n",
    "- You could visualize the predicted cursor locations versus the true cursor locations. What features does it predict well? What does it seem to miss?\n",
    "- You could visualized low-dimensional projections of the neural data for PCA versus DCA. Do you see any qualitative differences?\n",
    "- The cursor velocities are sometimes included as variables to predict in addition or as an alternative to the location. How does this change the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
